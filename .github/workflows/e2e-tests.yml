name: E2E Tests

on:
  push:
    branches: [main, feature/**]
  pull_request:
    branches: [main]
  workflow_dispatch:

jobs:
  e2e-tests:
    name: E2E Tests (Chromium)
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Install pnpm
        uses: pnpm/action-setup@v4
        with:
          version: 10

      - name: Get pnpm store directory
        shell: bash
        run: |
          echo "STORE_PATH=$(pnpm store path --silent)" >> $GITHUB_ENV

      - name: Setup pnpm cache
        uses: actions/cache@v5
        with:
          path: ${{ env.STORE_PATH }}
          key: ${{ runner.os }}-pnpm-store-${{ hashFiles('**/pnpm-lock.yaml') }}
          restore-keys: |
            ${{ runner.os }}-pnpm-store-

      - name: Install frontend dependencies
        working-directory: web-ui-dashboard
        run: pnpm install

      - name: Generate frontend config
        working-directory: web-ui-dashboard/src
        run: |
          echo '{"apiUrl": "http://localhost:8080"}' > config.json
          cat config.json

      - name: Build frontend
        working-directory: web-ui-dashboard
        run: pnpm build

      - name: Install Playwright browsers
        working-directory: web-ui-dashboard
        run: pnpm exec playwright install --with-deps chromium

      - name: Create test database
        run: |
          mkdir -p data
          python -c "
          from spark_optimizer.storage.database import Database
          from spark_optimizer.storage.models import SparkApplication  # Import models to register with Base
          db = Database('sqlite:///data/test_spark_optimizer.db')
          db.create_tables()
          print('Test database created successfully')
          "

      - name: Seed test data
        run: |
          python -c "
          from spark_optimizer.storage.database import Database
          from datetime import datetime, timedelta
          import random

          db = Database('sqlite:///data/test_spark_optimizer.db')

          # Create sample jobs for testing
          for i in range(20):
              job_data = {
                  'app_id': f'app-test-{i:03d}',
                  'app_name': f'TestJob{i}',
                  'user': 'test_user',
                  'submit_time': datetime.now() - timedelta(days=i),
                  'start_time': datetime.now() - timedelta(days=i, hours=1),
                  'end_time': datetime.now() - timedelta(days=i, hours=1, minutes=30),
                  'duration_ms': random.randint(300000, 3600000),
                  'status': random.choice(['completed', 'completed', 'completed', 'failed']),
                  'spark_version': '3.5.0',
                  'configuration': {
                      'executor_cores': random.choice([2, 4, 8]),
                      'executor_memory_mb': random.choice([4096, 8192, 16384]),
                      'num_executors': random.choice([5, 10, 20]),
                      'driver_memory_mb': random.choice([2048, 4096, 8192])
                  },
                  'metrics': {
                      'total_tasks': random.randint(100, 1000),
                      'failed_tasks': random.randint(0, 10),
                      'total_stages': random.randint(10, 50),
                      'failed_stages': 0,
                      'input_bytes': random.randint(1000000000, 100000000000),
                      'output_bytes': random.randint(1000000000, 50000000000),
                      'shuffle_read_bytes': random.randint(100000000, 10000000000),
                      'shuffle_write_bytes': random.randint(100000000, 10000000000)
                  }
              }
              db.save_job(job_data)

          print(f'Seeded {20} test jobs')
          "

      - name: Verify database file
        run: |
          echo "Checking database file..."
          ls -lh data/

          echo "Verifying database contents..."
          python -c "
          from spark_optimizer.storage.database import Database
          from spark_optimizer.storage.models import SparkApplication

          db = Database('sqlite:///data/test_spark_optimizer.db')

          with db.get_session() as session:
              count = session.query(SparkApplication).count()
              print(f'Database contains {count} jobs')

              if count > 0:
                  job = session.query(SparkApplication).first()
                  print(f'First job: {job.app_id} - {job.app_name}')
              else:
                  print('ERROR: No jobs found in database!')
                  exit(1)
          "

      - name: Start backend server
        run: |
          export DATABASE_URL=sqlite:///data/test_spark_optimizer.db
          python -m spark_optimizer.api.server &
          BACKEND_PID=$!
          echo "BACKEND_PID=$BACKEND_PID" >> $GITHUB_ENV

          # Wait for backend to be ready
          for i in {1..30}; do
            if curl -s http://localhost:8080/health > /dev/null; then
              echo "Backend server is ready!"
              break
            fi
            echo "Waiting for backend server... ($i/30)"
            sleep 2
          done

          if ! curl -s http://localhost:8080/health > /dev/null; then
            echo "Backend server failed to start"
            exit 1
          fi

      - name: Verify API endpoints
        run: |
          echo "Testing /api/v1/jobs endpoint..."
          JOBS_RESPONSE=$(curl -s http://localhost:8080/api/v1/jobs)
          echo "Response: $JOBS_RESPONSE"

          JOBS_COUNT=$(echo $JOBS_RESPONSE | python3 -c "import sys, json; data=json.load(sys.stdin); print(data.get('total', 0))")
          echo "Total jobs found: $JOBS_COUNT"

          if [ "$JOBS_COUNT" -lt 20 ]; then
            echo "ERROR: Expected at least 20 jobs, got $JOBS_COUNT"
            exit 1
          fi

          echo "âœ“ API endpoints verified successfully"

      - name: Run E2E tests
        working-directory: web-ui-dashboard
        env:
          CI: true
        run: pnpm exec playwright test --project=chromium --reporter=html

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: playwright-report-chromium
          path: web-ui-dashboard/playwright-report/
          retention-days: 7

      - name: Upload test videos
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: playwright-videos-chromium
          path: web-ui-dashboard/test-results/
          retention-days: 7

      - name: Stop backend server
        if: always()
        run: |
          if [ ! -z "$BACKEND_PID" ]; then
            kill $BACKEND_PID || true
          fi
          # Also kill any remaining Python processes on port 8080
          lsof -ti:8080 | xargs kill -9 || true
