# Data Collection

## Overview

The data collection layer is responsible for gathering Spark job metrics from various sources and normalizing them into a consistent format for storage and analysis.

## Collection Sources

### 1. Spark Event Logs

**Description**: Parse JSON event log files generated by Spark applications

**Use Cases**:
- Offline analysis of completed jobs
- Batch processing of historical data
- Local development and testing

**Advantages**:
- Complete job history
- Detailed event-level information
- No additional infrastructure required

**Limitations**:
- Requires access to event log storage
- Post-execution only (not real-time)
- Large file sizes for long-running jobs

**Configuration**:
```yaml
collector:
  type: event_log
  event_log_path: /path/to/spark/event-logs
  batch_size: 100
  recursive: true
```

**Example Usage**:
```python
from spark_optimizer.collectors.event_log_collector import EventLogCollector

collector = EventLogCollector(event_log_path="/path/to/logs")
jobs_data = collector.collect()
```

### 2. Spark History Server

**Description**: Query the Spark History Server REST API

**Use Cases**:
- Centralized Spark deployments
- Integration with existing infrastructure
- Real-time access to recent jobs

**Advantages**:
- No file system access needed
- API-based integration
- Aggregated metrics available

**Limitations**:
- Depends on History Server availability
- May have limited retention period
- Network latency considerations

**Configuration**:
```yaml
collector:
  type: history_server
  history_server_url: http://spark-history:18080
  timeout: 30
  max_apps: 1000
```

**Example Usage**:
```python
from spark_optimizer.collectors.history_server_collector import HistoryServerCollector

collector = HistoryServerCollector("http://spark-history:18080")
jobs_data = collector.collect()
```

### 3. Cloud Provider APIs

**Description**: Integrate with cloud provider APIs (AWS EMR, Databricks, GCP Dataproc)

**Use Cases**:
- Cloud-native Spark deployments
- Multi-cloud environments
- Managed Spark services

**Advantages**:
- Direct access to cluster metadata
- Cost information integration
- Cloud-specific optimizations
- Automatic discovery of jobs

**Limitations**:
- Requires cloud provider credentials
- API rate limits may apply
- Different APIs per provider

**Supported Providers**:
- **AWS EMR**: Uses boto3 to query EMR clusters and CloudWatch metrics
- **Databricks**: Integrates with Databricks REST API
- **GCP Dataproc**: Uses Google Cloud APIs for cluster and job data

**Example Usage**:
```python
from spark_optimizer.collectors.emr_collector import EMRCollector

collector = EMRCollector(region="us-west-2")
jobs_data = collector.collect()
```

For detailed cloud integration guides, see:
- [AWS EMR Integration](AWS_EMR_INTEGRATION.md)
- [Databricks Integration](DATABRICKS_INTEGRATION.md)
- [GCP Dataproc Integration](GCP_DATAPROC_INTEGRATION.md)

## Collected Metrics

### Application-Level Metrics

| Metric | Description | Source |
|--------|-------------|--------|
| `app_id` | Unique application identifier | All |
| `app_name` | Application name | All |
| `user` | User who submitted the job | All |
| `submit_time` | Job submission timestamp | All |
| `start_time` | Job start timestamp | All |
| `end_time` | Job completion timestamp | All |
| `duration_ms` | Total execution time | All |
| `spark_version` | Spark version used | Event Logs |

### Resource Configuration

| Metric | Description | Source |
|--------|-------------|--------|
| `executor_cores` | Cores per executor | All |
| `executor_memory_mb` | Memory per executor (MB) | All |
| `num_executors` | Number of executors | All |
| `driver_memory_mb` | Driver memory (MB) | All |

### Execution Metrics

| Metric | Description | Source |
|--------|-------------|--------|
| `total_tasks` | Total number of tasks | All |
| `failed_tasks` | Number of failed tasks | All |
| `total_stages` | Total number of stages | All |
| `failed_stages` | Number of failed stages | All |
| `cpu_time_ms` | Total CPU time | Event Logs, Metrics |

### Storage Metrics

| Metric | Description | Source |
|--------|-------------|--------|
| `input_bytes` | Total input data size | All |
| `output_bytes` | Total output data size | All |
| `shuffle_read_bytes` | Data read during shuffle | All |
| `shuffle_write_bytes` | Data written during shuffle | All |
| `memory_spilled_bytes` | Memory spilled to disk | Event Logs |
| `disk_spilled_bytes` | Disk spill amount | Event Logs |

## Collection Workflow

```
1. Initialize Collector
   ↓
2. Validate Configuration
   ↓
3. Connect to Data Source
   ↓
4. Fetch Raw Data
   ↓
5. Parse & Extract Metrics
   ↓
6. Normalize Data Format
   ↓
7. Validate Data Quality
   ↓
8. Return Structured Data
```

## Data Normalization

All collectors normalize data into a consistent format:

```python
{
    "app_id": "app-20240101-000001",
    "app_name": "example_job",
    "user": "data_engineer",
    "submit_time": "2024-01-01T10:00:00Z",
    "start_time": "2024-01-01T10:00:05Z",
    "end_time": "2024-01-01T10:05:05Z",
    "duration_ms": 300000,
    "spark_version": "3.4.0",

    "executor_cores": 4,
    "executor_memory_mb": 8192,
    "num_executors": 10,
    "driver_memory_mb": 4096,

    "total_tasks": 500,
    "failed_tasks": 0,
    "total_stages": 10,
    "failed_stages": 0,
    "cpu_time_ms": 1200000,

    "input_bytes": 10737418240,  # 10 GB
    "output_bytes": 5368709120,  # 5 GB
    "shuffle_read_bytes": 2147483648,  # 2 GB
    "shuffle_write_bytes": 2147483648,
    "memory_spilled_bytes": 0,
    "disk_spilled_bytes": 0,

    "tags": {"environment": "production", "team": "analytics"},
    "environment": {"SPARK_CONF": "..."}
}
```

## Batch Collection

For processing large volumes of historical data:

```python
from spark_optimizer.collectors.event_log_collector import EventLogCollector
from spark_optimizer.storage.database import Database
from spark_optimizer.storage.repository import SparkApplicationRepository

# Initialize
collector = EventLogCollector("/path/to/logs")
db = Database("sqlite:///spark_optimizer.db")
db.create_tables()

# Batch collect and store
with db.get_session() as session:
    repo = SparkApplicationRepository(session)

    for batch in collector.collect_batches(batch_size=100):
        for job_data in batch:
            # Check if job already exists
            existing = repo.get_by_app_id(job_data["app_id"])
            if not existing:
                repo.create(job_data)

        print(f"Processed batch of {len(batch)} jobs")
```

## Incremental Collection

For ongoing collection of new jobs:

```python
from datetime import datetime, timedelta

# Collect only jobs from the last 24 hours
since = datetime.now() - timedelta(days=1)

collector = HistoryServerCollector("http://spark-history:18080")
recent_jobs = collector.collect(since=since)
```

## Error Handling

### Common Issues

1. **Connection Failures**
   - Retry with exponential backoff
   - Log failure for monitoring
   - Return partial results

2. **Malformed Data**
   - Skip invalid records
   - Log validation errors
   - Continue with valid data

3. **Incomplete Metrics**
   - Use default values where appropriate
   - Mark data quality flags
   - Store available metrics

### Example Error Handling

```python
from spark_optimizer.collectors.base_collector import BaseCollector

class RobustCollector(BaseCollector):
    def collect(self):
        jobs = []
        errors = []

        try:
            raw_data = self._fetch_data()
        except ConnectionError as e:
            self.logger.error(f"Connection failed: {e}")
            return jobs, errors

        for item in raw_data:
            try:
                job_data = self._parse_job(item)
                if self._validate_job(job_data):
                    jobs.append(job_data)
                else:
                    errors.append({"item": item, "error": "validation_failed"})
            except Exception as e:
                errors.append({"item": item, "error": str(e)})

        return jobs, errors
```

## Performance Optimization

### 1. Parallel Processing

Process multiple event logs concurrently:

```python
from concurrent.futures import ThreadPoolExecutor

def process_log_file(log_path):
    collector = EventLogCollector(log_path)
    return collector.collect()

with ThreadPoolExecutor(max_workers=4) as executor:
    results = executor.map(process_log_file, log_files)
```

### 2. Caching

Cache parsed data to avoid redundant processing:

```python
import hashlib
import json

def get_cache_key(log_path, mtime):
    key_data = f"{log_path}:{mtime}"
    return hashlib.md5(key_data.encode()).hexdigest()

# Check cache before parsing
cache_key = get_cache_key(log_path, os.path.getmtime(log_path))
cached = cache.get(cache_key)
if cached:
    return json.loads(cached)
```

### 3. Streaming Parsing

For large event log files, use streaming:

```python
def parse_event_log_stream(file_path):
    with open(file_path, 'r') as f:
        for line in f:
            event = json.loads(line)
            yield process_event(event)
```

## Scheduling Collection

### Cron-based Collection

```bash
# Collect new jobs every hour
0 * * * * /usr/local/bin/spark-optimizer collect --event-log-dir /path/to/logs
```

### Programmatic Scheduling

```python
from apscheduler.schedulers.blocking import BlockingScheduler

def collect_job():
    collector = EventLogCollector("/path/to/logs")
    jobs = collector.collect()
    # Store jobs in database
    store_jobs(jobs)

scheduler = BlockingScheduler()
scheduler.add_job(collect_job, 'interval', hours=1)
scheduler.start()
```

## Data Quality Validation

### Required Fields

Ensure all critical fields are present:

```python
REQUIRED_FIELDS = [
    "app_id",
    "app_name",
    "start_time",
    "end_time",
    "duration_ms",
    "executor_cores",
    "num_executors"
]

def validate_job_data(job_data):
    missing = [f for f in REQUIRED_FIELDS if f not in job_data]
    if missing:
        raise ValueError(f"Missing required fields: {missing}")
    return True
```

### Data Ranges

Validate metric values are within reasonable ranges:

```python
def validate_ranges(job_data):
    if job_data["duration_ms"] < 0:
        raise ValueError("Duration cannot be negative")

    if job_data["num_executors"] < 1:
        raise ValueError("Must have at least 1 executor")

    if job_data["failed_tasks"] > job_data["total_tasks"]:
        raise ValueError("Failed tasks cannot exceed total tasks")
```

## Best Practices

1. **Incremental Collection**: Only collect new/changed data
2. **Deduplication**: Check for existing records before inserting
3. **Error Logging**: Log all errors with context for debugging
4. **Monitoring**: Track collection metrics (count, duration, errors)
5. **Retention**: Implement data retention policies
6. **Validation**: Validate data before storage
7. **Backpressure**: Handle rate limiting from APIs
8. **Idempotency**: Make collection operations idempotent

## CLI Usage

```bash
# Collect from event logs
spark-optimizer collect --source event-logs --path /path/to/logs

# Collect from History Server
spark-optimizer collect --source history-server --url http://localhost:18080

# Collect with filters
spark-optimizer collect --source event-logs --path /path/to/logs --since 2024-01-01 --user data_team

# Batch collection
spark-optimizer collect --source event-logs --path /path/to/logs --batch-size 50

# Dry run (validate without storing)
spark-optimizer collect --source event-logs --path /path/to/logs --dry-run
```
